{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name the coronary artery disease that is caused as a result of narrowing of '\n",
      " 'the lumen of arteries.',\n",
      " 'Explain the different forms of lipids with some examples.',\n",
      " 'What is the role of the time gap in the passage of action potential from the '\n",
      " 'sino-atrial node to the ventricle?',\n",
      " 'Explain through the Watson-Crick model, the secondary structure exhibited by '\n",
      " 'the nucleic acids.',\n",
      " 'What happens if the blood does not coagulate?',\n",
      " 'State the differences between the following: Lymph and blood',\n",
      " 'Why are thrombocytes necessary for blood coagulation?',\n",
      " 'Describe the Rh-incompatibility in humans.',\n",
      " 'Disadvantage of Computer ?',\n",
      " 'What are all the advantages of using a computer?',\n",
      " 'Give a reason why the walls of ventricles are thicker than atria.',\n",
      " 'what is computer ?',\n",
      " 'Can rubber be classified as a primary metabolite or a secondary metabolite? '\n",
      " 'Write a short note on the rubber.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "questions = []\n",
    "with open('ques_ans4.json', 'r') as fp:\n",
    "    data2 = json.load(fp)\n",
    "    for qap in data2['qa_pairs']:\n",
    "        #print(qap)\n",
    "        questions.append(qap['ques'])\n",
    "from pprint import pprint\n",
    "pprint(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "def getSynonyms(word1):\n",
    "    synonymList1 = []\n",
    "    for data1 in word1:\n",
    "        wordnetSynset1 = wn.synsets(data1)\n",
    "        tempList1=[]\n",
    "        tempList1.append(data1)\n",
    "        #print(wordnetSynset1)\n",
    "        \n",
    "        for synset1 in wordnetSynset1:\n",
    "            synLemmas = synset1.lemma_names()\n",
    "            for i in range(len(synLemmas)):\n",
    "                word = synLemmas[i].replace('_',' ')\n",
    "                if word not in tempList1:\n",
    "                    tempList1.append(word)\n",
    "        \n",
    "        spl = False            \n",
    "        if(data1 == \"what\"):\n",
    "            spl = True\n",
    "            wordnetSynset1=wn.synsets(\"define\")\n",
    "        elif(data1 == \"explain\"):\n",
    "            spl = True\n",
    "            wordnetSynset1=wn.synsets(\"describe\")            \n",
    "        elif(data1 == \"difference\"):\n",
    "            spl = True\n",
    "            wordnetSynset1=wn.synsets(\"distinguish\")\n",
    "        elif(data1 == \"advantage\"):\n",
    "            spl = True\n",
    "            tempList1.extend([\"pros\",\"merits\"])\n",
    "            wordnetSynset1=wn.synsets(\"benefits\")\n",
    "        elif(data1 == \"disadvantage\"):\n",
    "            spl = True\n",
    "            tempList1.append(\"cons\")\n",
    "            wordnetSynset1=wn.synsets(\"demerits\")\n",
    "        elif(data1 == \"demerits\"):\n",
    "            spl = True\n",
    "            tempList1.append(\"cons\")\n",
    "            wordnetSynset1=wn.synsets(\"disadvantage\")\n",
    "            \n",
    "        if(spl):    \n",
    "            for synset1 in wordnetSynset1:\n",
    "                synLemmas = synset1.lemma_names()\n",
    "                for i in range(len(synLemmas)):\n",
    "                    word = synLemmas[i].replace('_',' ')\n",
    "                    if word not in tempList1:\n",
    "                        tempList1.append(word)\n",
    "        synonymList1.append(tempList1)\n",
    "    return synonymList1\n",
    "\n",
    "#print(getSynonyms([\"demerits\"]))\n",
    "\n",
    "def getSynonymsIndex(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    \n",
    "    if(len1+len2 == 0):\n",
    "        return 0.5\n",
    "    \n",
    "    counter = 0\n",
    "    d2 = getSynonyms(word2)\n",
    "    #print(d2)\n",
    "    \n",
    "    for i in range(len(word1)):          \n",
    "        for k in range(len(d2)):\n",
    "            if word1[i] in d2[k]:\n",
    "                counter += 1\n",
    "                d2.pop(k)\n",
    "                break\n",
    "\n",
    "    syn_index = counter*2 / (len1 + len2)\n",
    "    return syn_index\n",
    "\n",
    "#print (getSynonymsIndex(word1, word2))     \n",
    "\n",
    "\n",
    "def getMatchingIndex(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    \n",
    "    if(len1+len2 == 0):\n",
    "        return 0.5\n",
    "    \n",
    "    counter = 0\n",
    "    for i in word1:\n",
    "        for j in word2:\n",
    "            if(i in j):\n",
    "                counter+=1\n",
    "                word2.remove(j)\n",
    "                break\n",
    "    match_index = counter*2 / (len1 + len2)\n",
    "    return match_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def preProcess(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub(r'[^A-Za-z\\s]+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "\n",
    "    #lemmatize\n",
    "    tokens = nltk.word_tokenize(sentence)  \n",
    "    #print(tokens)\n",
    "    sentence_tmp = \"\"\n",
    "    for word,pos in nltk.pos_tag(tokens):\n",
    "        if type(word) == str:\n",
    "            word = word.lower()\n",
    "            \n",
    "        if not pos == \"DT\":\n",
    "            #print(pos)\n",
    "            sentence_tmp+=(lemmatizer.lemmatize(word)) + \" \"  \n",
    "    #print(sentence_tmp)\n",
    "    return sentence_tmp\n",
    "\n",
    "def extract_features(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pre_features = [[],[]]\n",
    "\n",
    "    counter=-1\n",
    "    \n",
    "    #print(sentences)\n",
    "    for sentence in sentences:\n",
    "        counter+=1\n",
    "\n",
    "        sentence = preProcess(sentence)\n",
    "\n",
    "        nouns = [] #empty to array to hold all nouns\n",
    "        possesives = []\n",
    "        adverbs = []\n",
    "        adjectives = []\n",
    "        verbs = []\n",
    "        digrms = []\n",
    "        questionWords = []\n",
    "\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        for word,pos in nltk.pos_tag(tokens):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "            elif (pos == 'WDT' or pos == 'WP' or pos == 'WRB'):\n",
    "                questionWords.append(word)                 \n",
    "            elif (pos == 'POS'):\n",
    "                possesives.append(word)\n",
    "            elif (pos == 'RB' or pos == 'RBR' or pos == 'RBS'):\n",
    "                adverbs.append(word)\n",
    "            elif (pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "                adjectives.append(word)                \n",
    "            elif (pos == 'VB' or pos == 'VBD' or pos == 'VBG' or pos == 'VBN' or pos == 'VBP' or pos == 'VBZ'):\n",
    "                verbs.append(word)   \n",
    "\n",
    "        \n",
    "        bigrm = nltk.bigrams(tokens)\n",
    "        for i in bigrm:\n",
    "            digrms.append(\" \".join(i))\n",
    "        #print(digrms)\n",
    "        \n",
    "        \n",
    "        from nltk.corpus import stopwords  \n",
    "        stop_words = stopwords.words('english')\n",
    "        list_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        pre_features[counter].append(nouns) #0\n",
    "        pre_features[counter].append(possesives) #1\n",
    "        pre_features[counter].append(adverbs) #2\n",
    "        pre_features[counter].append(adjectives) #3\n",
    "        pre_features[counter].append(verbs) #4\n",
    "        pre_features[counter].append(digrms) #5\n",
    "        pre_features[counter].append(questionWords) #6\n",
    "        pre_features[counter].append(list(list_words)) #7\n",
    "\n",
    "\n",
    "\n",
    "    #print(pre_features[0])\n",
    "    #print(pre_features[1])\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for x in [0,2,3,4,6,7]:\n",
    "        features.append(getSynonymsIndex(pre_features[0][x], pre_features[1][x]))\n",
    "\n",
    "    for x in [1,5]:\n",
    "        features.append(getMatchingIndex(pre_features[0][x], pre_features[1][x]))\n",
    "\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel():\n",
    "    import pickle\n",
    "    filename = 'finalizedLR_model.sav'\n",
    "    #pickle.dump(modelLR, open(filename, 'wb'))\n",
    "     \n",
    "    # load the model from disk\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def predictOutput(q1,q2):\n",
    "    \n",
    "    model = loadModel()\n",
    "    \n",
    "    q=[q1,q2]\n",
    "    #print(\"hku\")\n",
    "    test_x = [extract_features(q)]\n",
    "    test_x = pd.DataFrame(test_x, columns = ['nouns', 'adverbs', 'adjectives',\n",
    "                                   'verbs', 'words', 'possesives',\n",
    "                                   'digrms', 'questionWords'])  \n",
    "    \n",
    "    #print(model.predict_proba(test_x)[0][1])\n",
    "    \n",
    "    #print(model.predict_proba(test_x)[0][1] >= 0.80)\n",
    "    #return (model.predict(test_x))\n",
    "    return ( model.predict_proba(test_x)[0][1] )\n",
    "\n",
    "\n",
    "def predictAns(q1,questions=questions):\n",
    "    qno_prob = {}\n",
    "    for ques in questions:\n",
    "        qno_prob[ques] = predictOutput(q1, ques)\n",
    "\n",
    "    sorted_x = sorted(qno_prob.items(), key=lambda kv: kv[1])\n",
    "    #print(sorted_x)\n",
    "    print(sorted_x[-1:-3:-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what is computer ?', 0.8136820913104225), ('Disadvantage of Computer ?', 0.7792576804479737)]\n"
     ]
    }
   ],
   "source": [
    "q1 = \"define computer ?\"\n",
    "\n",
    "predictAns(q1,questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
